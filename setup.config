[metadata]
name = llm-uncertainty-evals
version = 0.1.0
description = Python package for measuring uncertainty in LLM responses and measuring the context usability in RAG responses, inspired by CLUE (https://arxiv.org/abs/2409.03021)
long_description = file: README.md
long_description_content_type = text/markdown
author = Nikilp
license = MIT

[options]
packages = find: